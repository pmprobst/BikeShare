comment = "#>",
class.output = "output",
class.message = "message"
)
library(sensitivitymw)
data(erpcp)
head(erpcp)
senmw(erpcp, gamma = 1, method = 't')$pval
senmwCI(erpcp, gamma = 1, method = 't', one.sided = TRUE)
gamma_vals <- seq(from = 1., to = 5.0, by = 0.4)
sensitivity_output <- data.frame('gamma' = gamma_vals,
'p-value' = NA,
'CI.L' = NA,
'CI.U' = NA)
for(i in 1:length(gamma_vals)){
gamma_val <- gamma_vals[i]
sensitivity_output[i,2] <- senmw(erpcp,
gamma = gamma_val,
method = 't')$pval
sensitivity_output[i,3:4] <- senmwCI(erpcp,
gamma = gamma_val,
method = 't')$Confidence.Interval
}
library(kableExtra)
sensitivity_output |>
kable()
library(sensitivitymv)
Delta_seq <- c(5:15)
Lambda_seq <- sapply(Delta_seq,  function(x) amplify(3.8, x))
plot(Lambda_seq,
Delta_seq,
pch = 20,
main = 'Amplification of Gamma = 3.8',
xlab = 'Lambda',
ylab = 'Delta',
xlim = c(0,15),
yli = c(0,15))
abline(h = 3.8, lty = 'dotted')
abline(v = 3.8, lty = 'dotted')
amplify(3.8,7)
head(tbmetaphase)
gamma_vals <- seq(from = 1, to = 4.0, by = 0.2)
sensitivity_output <- data.frame('gamma' = gamma_vals,
'p-value' = NA)
for(i in 1:length(gamma_vals)){
gamma_val <- gamma_vals[i]
sensitivity_output[i,2] <- senmv(tbmetaphase,
gamma = gamma_val,
method = 't', TonT = TRUE)$pval
}
sensitivity_output |>
kable()
# Chunk 1
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>",
class.output = "output",
class.message = "message"
)
# Chunk 2
library(sensitivitymw)
data(erpcp)
head(erpcp)
senmw(erpcp, gamma = 1, method = 't')$pval
# Chunk 3
senmwCI(erpcp, gamma = 1, method = 't', one.sided = TRUE)
# Chunk 4
gamma_vals <- seq(from = 1., to = 5.0, by = 0.4)
sensitivity_output <- data.frame('gamma' = gamma_vals,
'p-value' = NA,
'CI.L' = NA,
'CI.U' = NA)
for(i in 1:length(gamma_vals)){
gamma_val <- gamma_vals[i]
sensitivity_output[i,2] <- senmw(erpcp,
gamma = gamma_val,
method = 't')$pval
sensitivity_output[i,3:4] <- senmwCI(erpcp,
gamma = gamma_val,
method = 't')$Confidence.Interval
}
library(kableExtra)
sensitivity_output |>
kable()
# Chunk 5
library(sensitivitymv)
Delta_seq <- c(5:15)
Lambda_seq <- sapply(Delta_seq,  function(x) amplify(3.8, x))
plot(Lambda_seq,
Delta_seq,
pch = 20,
main = 'Amplification of Gamma = 3.8',
xlab = 'Lambda',
ylab = 'Delta',
xlim = c(0,15),
yli = c(0,15))
abline(h = 3.8, lty = 'dotted')
abline(v = 3.8, lty = 'dotted')
amplify(3.8,7)
# Chunk 6
head(tbmetaphase)
# Chunk 7
gamma_vals <- seq(from = 1, to = 4.0, by = 0.2)
sensitivity_output <- data.frame('gamma' = gamma_vals,
'p-value' = NA)
for(i in 1:length(gamma_vals)){
gamma_val <- gamma_vals[i]
sensitivity_output[i,2] <- senmv(tbmetaphase,
gamma = gamma_val,
method = 't', TonT = TRUE)$pval
}
sensitivity_output |>
kable()
# Chunk 1
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>",
class.output = "output",
class.message = "message"
)
# Chunk 2
rm(list = ls())
embed_png <- function(path, dpi = NULL) {
meta <- attr(png::readPNG(path, native = TRUE, info = TRUE), "info")
if (!is.null(dpi)) meta$dpi <- rep(dpi, 2)
knitr::asis_output(paste0(
"<img src='", path, "'",
" width=", round(meta$dim[1] / (meta$dpi[1] / 96)),
" height=", round(meta$dim[2] / (meta$dpi[2] / 96)),
" style='display: block; margin: auto;'/>"
))
}
# Chunk 3
# loads package
library(sensemakr)
# loads data
data("darfur")
# Chunk 4
# runs regression model
darfur.model <- lm(peacefactor ~ directlyharmed  + village +  female +
age + farmer_dar + herder_dar + pastvoted + hhsize_darfur,
data = darfur)
# Chunk 5
stargazer::stargazer(darfur.model, keep = "directlyharmed", type = "text")
# Chunk 7
# runs sensemakr for sensitivity analysis
# in the darfur example
darfur.sensitivity <- sensemakr(model = darfur.model,
treatment = "directlyharmed",
benchmark_covariates = "female",
kd = 1:3,
ky = 1:3,
q = 1,
alpha = 0.05,
reduce = TRUE)
# Chunk 8
darfur.sensitivity <- sensemakr(model = darfur.model,
treatment = "directlyharmed",
benchmark_covariates = "female",
kd = 1:3)
# Chunk 9
darfur.sensitivity
# Chunk 10
ovb_minimal_reporting(darfur.sensitivity, format = "html")
# Chunk 11
summary(darfur.sensitivity)
# Chunk 12
plot(darfur.sensitivity)
# Chunk 13
plot(darfur.sensitivity, sensitivity.of = "t-value")
# Chunk 14
plot(darfur.sensitivity, type = "extreme")
# Chunk 15
# loads sensemakr package
# simulates data
n <- 100
X <- scale(rnorm(n))
Z <- resid_maker(n, X)
D <- X + Z + resid_maker(n, cbind(X, Z))
Y <- X + Z + resid_maker(n, cbind(X, Z, D))
# Chunk 16
model.ydx <- lm(Y ~ D + X)
summary(model.ydx)
# Chunk 17
# fits treatment regression
model.dx <- lm(D ~ X)
# computes observed partial R2 of X
r2yx.d <- partial_r2(model.ydx, covariates = "X")
r2dx   <- partial_r2(model.dx, covariates = "X")
# Chunk 18
informal_adjusted_estimate <- adjusted_estimate(model.ydx,
treatment = "D",
r2dz.x = r2dx,
r2yz.dx = r2yx.d)
# Chunk 19
# draws sensitivity contours
ovb_contour_plot(model.ydx,
treatment = "D",
lim = .6)
# adds informal benchmark
add_bound_to_contour(r2dz.x = r2dx,
r2yz.dx = r2yx.d,
bound_value = informal_adjusted_estimate,
bound_label = "Informal benchmark")
# Chunk 20
# compute formal bounds
formal_bound <- ovb_bounds(model = model.ydx,
treatment = "D",
benchmark_covariates = "X",
kd = 1, ky = 1)
# Chunk 21
# contour plot
ovb_contour_plot(model.ydx,
treatment = "D",
lim = .6)
add_bound_to_contour(r2dz.x = r2dx,
r2yz.dx = r2yx.d,
bound_value = informal_adjusted_estimate,
bound_label = "Informal benchmark")
add_bound_to_contour(bounds = formal_bound,
bound_label = "Proper bound")
train_data <- vroom("data/train.csv") %>%
select(-casual, -registered)
# BikeShare Penalized Regression Analysis
# Load required libraries
library(tidyverse)
library(tidymodels)
library(dplyr)
library(rpart)
library(vroom)
library(ranger)
library(bonsai)
library(lightgbm)
library(parsnip)
setwd("~/Library/CloudStorage/OneDrive-BrighamYoungUniversity/STAT 348/Repos/BikeShare")
train_data <- vroom("data/train.csv") %>%
select(-casual, -registered)
test_data <- vroom("data/test.csv")
my_recipe <- recipe(count ~ . ,data = train_data) %>%
step_log(count, offset = 1, skip = TRUE) %>%
# Recode weather "4" to "3" (combine rare weather conditions)
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
# Extract hour from datetime
step_time(datetime, features = "hour") %>%
# Day of week
step_mutate(day_of_week = wday(datetime, label = TRUE)) %>%
# Month of year
step_mutate(month = month(datetime)) %>%
# Weekend vs weekday
step_mutate(is_weekend = ifelse(wday(datetime) %in% c(1, 7), 1, 0)) %>%
# Rush hour indicators
step_mutate(is_morning_rush = ifelse(hour(datetime) %in% 7:9, 1, 0)) %>%
step_mutate(is_evening_rush = ifelse(hour(datetime) %in% 17:19, 1, 0)) %>%
# Time of day categories
step_mutate(time_of_day = case_when(
hour(datetime) %in% 6:11 ~ "morning",
hour(datetime) %in% 12:17 ~ "afternoon",
hour(datetime) %in% 18:22 ~ "evening",
TRUE ~ "night"
)) %>%
# Temperature-humidity interaction
step_mutate(temp_humidity = temp * humidity) %>%
# Wind speed categories
step_mutate(wind_category = case_when(
windspeed < 10 ~ "calm",
windspeed < 20 ~ "moderate",
TRUE ~ "windy"
)) %>%
# Temperature bins
step_discretize(temp, num_breaks = 4) %>%
# Humidity bins
step_discretize(humidity, num_breaks = 3) %>%
# Quadratic terms for continuous variables
# (using atemp and windspeed to avoid conflicts)
step_poly(atemp, degree = 2) %>%
step_poly(windspeed, degree = 2) %>%
# Distance from holidays
step_mutate(days_from_holiday = abs(as.numeric(as.Date(datetime) -
as.Date("2011-01-01")))) %>%
# Special day indicators
step_mutate(is_special_day = ifelse(holiday == 1 | workingday == 0, 1, 0)) %>%
# Make weather a factor after recoding
step_mutate(weather = factor(weather)) %>%
# Make season a factor
step_mutate(season = factor(season)) %>%
# Make time_of_day a factor
step_mutate(time_of_day = factor(time_of_day)) %>%
# Make wind_category a factor
step_mutate(wind_category = factor(wind_category)) %>%
# Weather-temperature interaction (using numeric weather and temp)
step_mutate(weather_temp = as.numeric(weather) * temp) %>%
# Season-hour interaction (using numeric season)
step_mutate(season_hour = as.numeric(season) * hour(datetime)) %>%
# Season-weather interaction (using numeric values)
step_mutate(season_weather = as.numeric(season) * as.numeric(weather)) %>%
# Create dummy variables for all nominal predictors
#(encodes all categorical variables)
step_dummy(all_nominal_predictors()) %>%
# Remove highly correlated features to reduce multicollinearity
step_corr(all_numeric_predictors(), threshold = 0.95) %>%
# Remove near-zero variance predictors
step_nzv(all_predictors()) %>%
# Normalize all numeric predictors to put them on the same scale
step_normalize(all_numeric_predictors()) %>%
# Remove datetime column as it's not needed for modeling
step_rm(datetime)
# Bake the data
baked_data <- bake(prepped, new_data = train_data)
# Bake the data
baked_data <- bake(my_recipe, new_data = train_data)
packages.install("agua")
package.install("agua")
install.packages("agua")
library(agua)
## Initialize an h2o session
h2o::h2o.init()
train_data <- vroom("data/train.csv") %>%
select(-casual, -registered)
test_data <- vroom("data/test.csv")
my_recipe <- recipe(count ~ . ,data = train_data) %>%
step_log(count, offset = 1, skip = TRUE) %>%
# Recode weather "4" to "3" (combine rare weather conditions)
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
# Extract hour from datetime
step_time(datetime, features = "hour") %>%
# Day of week
step_mutate(day_of_week = wday(datetime, label = TRUE)) %>%
# Month of year
step_mutate(month = month(datetime)) %>%
# Weekend vs weekday
step_mutate(is_weekend = ifelse(wday(datetime) %in% c(1, 7), 1, 0)) %>%
# Rush hour indicators
step_mutate(is_morning_rush = ifelse(hour(datetime) %in% 7:9, 1, 0)) %>%
step_mutate(is_evening_rush = ifelse(hour(datetime) %in% 17:19, 1, 0)) %>%
# Time of day categories
step_mutate(time_of_day = case_when(
hour(datetime) %in% 6:11 ~ "morning",
hour(datetime) %in% 12:17 ~ "afternoon",
hour(datetime) %in% 18:22 ~ "evening",
TRUE ~ "night"
)) %>%
# Temperature-humidity interaction
step_mutate(temp_humidity = temp * humidity) %>%
# Wind speed categories
step_mutate(wind_category = case_when(
windspeed < 10 ~ "calm",
windspeed < 20 ~ "moderate",
TRUE ~ "windy"
)) %>%
# Temperature bins
step_discretize(temp, num_breaks = 4) %>%
# Humidity bins
step_discretize(humidity, num_breaks = 3) %>%
# Quadratic terms for continuous variables
# (using atemp and windspeed to avoid conflicts)
step_poly(atemp, degree = 2) %>%
step_poly(windspeed, degree = 2) %>%
# Distance from holidays
step_mutate(days_from_holiday = abs(as.numeric(as.Date(datetime) -
as.Date("2011-01-01")))) %>%
# Special day indicators
step_mutate(is_special_day = ifelse(holiday == 1 | workingday == 0, 1, 0)) %>%
# Make weather a factor after recoding
step_mutate(weather = factor(weather)) %>%
# Make season a factor
step_mutate(season = factor(season)) %>%
# Make time_of_day a factor
step_mutate(time_of_day = factor(time_of_day)) %>%
# Make wind_category a factor
step_mutate(wind_category = factor(wind_category)) %>%
# Weather-temperature interaction (using numeric weather and temp)
step_mutate(weather_temp = as.numeric(weather) * temp) %>%
# Season-hour interaction (using numeric season)
step_mutate(season_hour = as.numeric(season) * hour(datetime)) %>%
# Season-weather interaction (using numeric values)
step_mutate(season_weather = as.numeric(season) * as.numeric(weather)) %>%
# Create dummy variables for all nominal predictors
#(encodes all categorical variables)
step_dummy(all_nominal_predictors()) %>%
# Remove highly correlated features to reduce multicollinearity
step_corr(all_numeric_predictors(), threshold = 0.95) %>%
# Remove near-zero variance predictors
step_nzv(all_predictors()) %>%
# Normalize all numeric predictors to put them on the same scale
step_normalize(all_numeric_predictors()) %>%
# Remove datetime column as it's not needed for modeling
step_rm(datetime)
auto_model <- auto_ml() %>%
set_engine("h2o" ,max_runtime_secs = 500 ,max_models = 10) %>%
set_mode("regression")
# Establish a workflow
wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(auto_model) %>%
fit(data = train_data)
# predict and back-transform from log1p, clip at 0
test_predictions <- predict(wf, new_data = test_data) %>%
mutate(.pred = pmax(0, exp(.pred) - 1))
# Create Kaggle submission format (preserve original datetime strings)
kaggle_submission <- test_predictions %>%
bind_cols(., test_data) %>%
select(datetime, .pred) %>%
rename(count = .pred) %>%
mutate(count = pmax(0, count)) %>%
mutate(datetime = as.character(format(datetime)))
# Write to CSV file
vroom_write(x = kaggle_submission, file = "./H2O_Predictions.csv", delim = ",")
my_recipe <- recipe(count ~ . ,data = train_data) %>%
step_log(count, offset = 1, skip = TRUE) %>%
# Recode weather "4" to "3" (combine rare weather conditions)
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
# Extract hour from datetime
step_time(datetime, features = "hour") %>%
# Day of week
step_date(datetime, features = "dow") %>%
# Month of year
step_mutate(month = month(datetime)) %>%
# Weekend vs weekday
step_mutate(is_weekend = ifelse(wday(datetime) %in% c(1, 7), 1, 0)) %>%
# Rush hour indicators
step_mutate(is_morning_rush = ifelse(hour(datetime) %in% 7:9, 1, 0)) %>%
step_mutate(is_evening_rush = ifelse(hour(datetime) %in% 17:19, 1, 0)) %>%
# Time of day categories
step_mutate(time_of_day = case_when(
hour(datetime) %in% 6:11 ~ "morning",
hour(datetime) %in% 12:17 ~ "afternoon",
hour(datetime) %in% 18:22 ~ "evening",
TRUE ~ "night"
)) %>%
# Temperature-humidity interaction
step_mutate(temp_humidity = temp * humidity) %>%
# Wind speed categories
step_mutate(wind_category = case_when(
windspeed < 10 ~ "calm",
windspeed < 20 ~ "moderate",
TRUE ~ "windy"
)) %>%
# Temperature bins
step_discretize(temp, num_breaks = 4) %>%
# Humidity bins
step_discretize(humidity, num_breaks = 3) %>%
# Quadratic terms for continuous variables
# (using atemp and windspeed to avoid conflicts)
step_poly(atemp, degree = 2) %>%
step_poly(windspeed, degree = 2) %>%
# Distance from holidays
step_mutate(days_from_holiday = abs(as.numeric(as.Date(datetime) -
as.Date("2011-01-01")))) %>%
# Special day indicators
step_mutate(is_special_day = ifelse(holiday == 1 | workingday == 0, 1, 0)) %>%
# Make weather a factor after recoding
step_mutate(weather = factor(weather)) %>%
# Make season a factor
step_mutate(season = factor(season)) %>%
# Make time_of_day a factor
step_mutate(time_of_day = factor(time_of_day)) %>%
# Make wind_category a factor
step_mutate(wind_category = factor(wind_category)) %>%
# Weather-temperature interaction (using numeric weather and temp)
step_mutate(weather_temp = as.numeric(weather) * temp) %>%
# Season-hour interaction (using numeric season)
step_mutate(season_hour = as.numeric(season) * hour(datetime)) %>%
# Season-weather interaction (using numeric values)
step_mutate(season_weather = as.numeric(season) * as.numeric(weather)) %>%
# Create dummy variables for all nominal predictors
#(encodes all categorical variables)
step_dummy(all_nominal_predictors()) %>%
# Remove highly correlated features to reduce multicollinearity
step_corr(all_numeric_predictors(), threshold = 0.95) %>%
# Remove near-zero variance predictors
step_nzv(all_predictors()) %>%
# Normalize all numeric predictors to put them on the same scale
step_normalize(all_numeric_predictors()) %>%
#update date_time to ID
update_role(datetime, new_role = "ID")
baked_data <- bake(my_recipe, new_data = train_data)
# Prep the recipe
prepped_recipe <- prep(my_recipe, training = train_data)
# Bake the data
baked_data <- bake(prepped_recipe, new_data = train_data)
vroom_write(x = baked_data, file = "./Baked_Data.csv", delim = ",")
View(baked_data)
View(baked_data)
# Prep the recipe
prepped_recipe <- prep(my_recipe, training = test_data)
# Prep the recipe
prepped_recipe <- prep(my_recipe, training = train_data)
# Bake the data
baked_data <- bake(prepped_recipe, new_data = test_data)
vroom_write(x = baked_data, file = "./Baked_Test_Data.csv", delim = ",")
pred_data <- vroom("data/DataRobotResults.csv")
kaggle_submission <- pred_data %>%
bind_cols(., test_data) %>%
select(datetime, count_PREDICTION) %>%
rename(count = count_PREDICTION) %>%
mutate(count = pmax(0, count)) %>%
mutate(datetime = as.character(format(datetime)))
pred_data <- vroom("data/DataRobotResults.csv")
kaggle_submission <- pred_data %>%
bind_cols(., test_data) %>%
select(datetime, count_PREDICTION) %>%
rename(count = count_PREDICTION) %>%
mutate(count = pmax(0, count)) %>%
mutate(datetime = as.character(format(datetime)))
kaggle_submission <- pred_data %>%
#bind_cols(., pred_data) %>%
select(datetime, count_PREDICTION) %>%
rename(count = count_PREDICTION) %>%
mutate(count = pmax(0, count)) %>%
mutate(datetime = as.character(format(datetime)))
View(kaggle_submission)
vroom_write(x = kaggle_submission, file = "./DataRobot_Solution.csv", delim = ",")
